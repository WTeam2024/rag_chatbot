{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WTeam2024/rag_chatbot/blob/main/chatbot_WTeam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install required packages\n",
        "!pip install -q peft  accelerate bitsandbytes safetensors sentencepiece streamlit chromadb langchain sentence-transformers gradio pypdf langchain-community\n",
        "!pip install -U transformers\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "V4765SUu_xeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing unicode error in google colab\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "5U2oe5Nh_y8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n",
        "import os\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "import chromadb\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "PDGQHes2_5Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify model huggingface mode name\n",
        "model_name = \"galatolo/cerbero-7b\""
      ],
      "metadata": {
        "id": "z8Nmpda6ATlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for loading 4-bit quantized model\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "mNlD2KDzAXm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fucntion for initializing tokenizer\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "SfENIWjEAhdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "ySiRxdirJjtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install huggingface_hub\n",
        "# from huggingface_hub import *\n",
        "\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# # Login to Hugging Face Hub\n",
        "# #huggingfacehub_api_token = 'HF_TOKEN'\n",
        "# #login(token=huggingfacehub_api_token)\n",
        "\n",
        "# # Caricare il tokenizer e il modello\n",
        "# repo_id = 'tiiuae/falcon-7b-instruct'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b-instruct\")"
      ],
      "metadata": {
        "id": "3xHp-U36KHXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### PER FAR FUNZIONARE QUESTA PARTE: VAI SU RUNTIME -> CHANGE RUNTIME TYPE -> SELEZIONA T4 GPU!\n",
        "\n",
        "# load model\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# specify stop token ids\n",
        "stop_token_ids = [0]"
      ],
      "metadata": {
        "id": "Te-UH3O1EUjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive and specify folder path\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/MyDrive/documents_colab/'"
      ],
      "metadata": {
        "id": "sAjYf7RmAk3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SPOSTA I PDFs IN UN'UNICA CARTELLA\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Specifica la directory di partenza e la directory di destinazione\n",
        "source_dir = folder_path\n",
        "destination_dir = '/content/PDFs'\n",
        "\n",
        "# Controlla se la directory di destinazione esiste, altrimenti creala\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "\n",
        "# Funzione per spostare i file PDF\n",
        "def move_pdfs(source, destination):\n",
        "    for root, dirs, files in os.walk(source):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                source_file = os.path.join(root, file)\n",
        "                destination_file = os.path.join(destination, file)\n",
        "                shutil.copy(source_file, destination_file)\n",
        "                print(f'Spostato: {source_file} -> {destination_file}')\n",
        "\n",
        "# Esegui la funzione\n",
        "move_pdfs(source_dir, destination_dir)\n",
        "folder_path = destination_dir"
      ],
      "metadata": {
        "id": "lF2l_5nKs1JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pdf files\n",
        "loader = PyPDFDirectoryLoader(folder_path)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "hO9YskYDAwEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the documents in small chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200) #Chage the chunk_size and chunk_overlap as needed\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# Check the first few splits to ensure coherence\n",
        "for i, split in enumerate(all_splits[:3]):\n",
        "    print(f\"Chunk {i+1}:\\n{split}\\n{'-'*20}\")"
      ],
      "metadata": {
        "id": "gQyWSop1ocT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify embedding model (using huggingface sentence transformer)\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "#device = torch.device('cpu')\n",
        "model_kwargs = {\"device\": \"cuda\"}"
      ],
      "metadata": {
        "id": "wNgu4J2iofJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "#embed document chunks\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "# specify the retriever\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "O-m8Cr5rKfB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build huggingface pipeline for using zephyr-7b-alpha\n",
        "pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=2048,\n",
        "        do_sample=True,\n",
        "        top_k=5,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# specify the llm\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "PR06DLiVC2Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build conversational retrieval chain with memory (rag) using langchain\n",
        "def create_conversation(query: str, chat_history: list) -> tuple:\n",
        "    try:\n",
        "\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history',\n",
        "            return_messages=False\n",
        "        )\n",
        "        qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            memory=memory,\n",
        "            get_chat_history=lambda h: h,\n",
        "        )\n",
        "\n",
        "        result = qa_chain({'question': query, 'chat_history': chat_history})\n",
        "        chat_history.append((query, result['answer']))\n",
        "        return '', chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history.append((query, e))\n",
        "        return '', chat_history"
      ],
      "metadata": {
        "id": "ZEGk5ngk8yX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build gradio ui\n",
        "#with gr.Blocks() as demo:\n",
        "\n",
        "#    gr.Markdown(\"<h1 align='center'>Personal Chatbot</h1>\")\n",
        "#    chatbot = gr.Chatbot(label='Textmining Project - W_Team - 2023/2024')\n",
        "#    state = gr.State([])  # Cronologia della chat\n",
        "\n",
        "#    with gr.Row():\n",
        "#        with gr.Column(scale=9):  # La casella di testo occupa la maggior parte della larghezza\n",
        "#            msg = gr.Textbox(show_label=False, placeholder=\"Enter question and press enter\",lines=2)\n",
        "#        with gr.Column(scale=1):  # Il pulsante occupa una parte minore della larghezza\n",
        "#            submit_btn = gr.Button(\"Send\",size=\"sm\")\n",
        "#            clear_btn = gr.Button(\"Clear\", size=\"sm\")\n",
        "\n",
        "    #clear = gr.ClearButton([msg, chatbot])\n",
        "#    clear_btn.click(fn=lambda: (\"\", []), inputs=[], outputs=[msg, chatbot])\n",
        "#    submit_btn.click(fn=create_conversation, inputs=[msg, chatbot], outputs=[msg, chatbot],concurrency_limit=1)\n",
        "    #submit_btn.click(fn=create_conversation, inputs=msg, outputs=gr.ChatInterface, concurrency_limit=1)\n",
        "#    msg.submit(create_conversation, [msg, chatbot], [msg, chatbot],concurrency_limit=1)\n",
        "\n",
        "#demo.launch()"
      ],
      "metadata": {
        "id": "U9C0EkJSDr2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "    gr.Markdown(\"<h1 align='center'>Personal Chatbot</h1>\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    #msg = gr.Textbox(label=\"Your message:\")\n",
        "    state = gr.State([])  # Cronologia della chat\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(show_label=False, placeholder=\"Enter question and press enter\")\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "    gr.themes.Soft()\n",
        "\n",
        "    msg.submit(create_conversation, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True,share=True)"
      ],
      "metadata": {
        "id": "KnOSMjEsIzpo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}